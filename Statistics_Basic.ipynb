{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ntElXu5xG_-M"
      },
      "outputs": [],
      "source": [
        "1.  What is statistics, and why is it important\n",
        "Ans - Statistics is a branch of mathematics that deals with the collection, organization, analysis, interpretation, and presentation of data.\n",
        "It provides methods to summarize data meaningfully and draw conclusions or make decisions based on that data.\n",
        " Statistics is  Important because\n",
        "\n",
        "* Informed Decision-Making: Statistics helps individuals and organizations make data-driven decisions in areas like business, healthcare, education, and public policy.\n",
        "\n",
        "* Understanding Trends: It allows us to identify patterns, trends, and relationships in data, which can be crucial for forecasting and planning.\n",
        "\n",
        "* Research and Development: In scientific research, statistics is essential for designing experiments, testing hypotheses, and validating results.\n",
        "\n",
        "* Quality Control: In industries, statistical methods are used to monitor and improve product quality and performance."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.  What are the two main types of statistics\n",
        "Ans - 1. Descriptive Statistics:\n",
        "Descriptive statistics involve methods for summarizing and organizing data in a meaningful way. This includes:\n",
        "Measures of Central Tendency: Mean, median, and mode.\n",
        "Measures of Dispersion: Range, variance, and standard deviation.\n",
        "     2 . Inferential Statistics:\n",
        "Inferential statistics involve techniques that use a sample to make generalizations or predictions about a larger population. This includes:\n",
        "Hypothesis Testing.\n",
        "Confidence Intervals.\n",
        "Regression Analysis.\n",
        "ANOVA (Analysis of Variance).\n",
        "\n"
      ],
      "metadata": {
        "id": "VXJ567qFHtDr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.  What are descriptive statistics\n",
        "Ans - Descriptive Statistics are statistical methods that summarize and organize data so it can be easily understood. They help in describing the main features of a data set through numerical and graphical methods, without drawing conclusions beyond the data.\n",
        "\n"
      ],
      "metadata": {
        "id": "AJe8jCZLIZyM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.  What is inferential statistics\n",
        "Ans - nferential Statistics refers to the branch of statistics that allows us to make predictions, inferences, or generalizations about a population based on data collected from a sample of that population."
      ],
      "metadata": {
        "id": "GczkssdPIj10"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.  What is sampling in statistics\n",
        "Ans - Sampling in statistics is the process of selecting a subset (called a sample) from a larger group (called a population) to analyze and make inferences about the entire population."
      ],
      "metadata": {
        "id": "w-OHGjSEIs_Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.  What are the different types of sampling methods\n",
        "Ans - There are two main categories of sampling methods in statistics: probability sampling and non-probability sampling.\n",
        "     A. Probability Sampling Methods :\n",
        "In these methods, every member of the population has a known and non-zero chance of being selected. These are more statistically reliable and suitable for generalizing to the population.\n",
        " 1. Simple Random Sampling\n",
        "Every individual has an equal chance of being selected.\n",
        "Selection is often done using random number tables or software.\n",
        "Example: Drawing 50 student names from a hat containing all students’ names.\n",
        " 2. Systematic Sampling\n",
        "Selects every kth member from a list after a random start.\n",
        "Example: Choosing every 10th person on a class roll after randomly picking the first.\n",
        " 3. Stratified Sampling\n",
        "The population is divided into strata (subgroups) based on a characteristic (e.g., age, gender), and a random sample is taken from each stratum.\n",
        "Ensures representation from all subgroups.\n",
        "Example: Sampling 30 males and 30 females from a college.\n",
        " 4. Cluster Sampling\n",
        "The population is divided into clusters, usually geographically, and whole clusters are randomly selected.\n",
        "Cost-effective for large populations spread over a wide area.\n",
        "Example: Randomly selecting 5 schools and surveying all students in them.\n",
        "     B. Non-Probability Sampling Methods:\n",
        "These methods do not give every individual a known or equal chance of selection. They are easier and cheaper but less reliable for generalization.\n",
        " 1. Convenience Sampling\n",
        "Sample is taken from a group that is easy to access.\n",
        "Example: Surveying people in a nearby coffee shop.\n",
        " 2. Judgmental or Purposive Sampling\n",
        "Researcher selects individuals based on their knowledge or judgment.\n",
        "Example: Selecting experts for a study on climate change."
      ],
      "metadata": {
        "id": "Pl7lNiq4I6LM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.  What is the difference between random and non-random sampling\n",
        "Ans - * Random Sampling (Probability Sampling)\n",
        "Definition:\n",
        "In random sampling, every individual in the population has a known and non-zero chance of being selected. The selection is made using random methods (like random number generators, lotteries, etc.), ensuring that the sample is unbiased and representative of the population.\n",
        "     *Non-Random Sampling (Non-Probability Sampling)\n",
        "Definition:\n",
        "In non-random sampling, some members of the population have no chance of being selected, or the chances are unknown. The selection is often based on subjective judgment or convenience, making the sample potentially biased."
      ],
      "metadata": {
        "id": "dSY61EwKJz2j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.  Define and give examples of qualitative and quantitative data\n",
        "Ans -  Qualitative Data (Categorical Data)\n",
        "Definition:\n",
        "* Qualitative data describes characteristics, attributes, or categories that cannot be measured numerically. It is non-numeric and often used to classify or label data into distinct groups.\n",
        "Types:\n",
        "Nominal Data: Categories with no inherent order (e.g., gender, religion).\n",
        "Ordinal Data: Categories with a meaningful order but no fixed intervals (e.g., rankings, education levels).\n",
        "Examples:\n",
        "Colors of cars: Red, Blue, Black.\n",
        "Types of cuisine: Italian, Chinese, Indian.\n",
        " * Quantitative Data (Numerical Data)\n",
        "Definition:\n",
        "Quantitative data consists of values that are numerical and can be measured or counted. It represents quantities and allows for mathematical operations.\n",
        "Types:\n",
        "Discrete Data: Countable numbers (e.g., number of students, number of cars).\n",
        "Continuous Data: Measurable quantities that can take any value within a range (e.g., height, weight, temperature).\n",
        "Examples:\n",
        "Age of students: 18, 20, 22.\n",
        "Temperature: 36.5°C, 40.0°C.\n",
        "Salary: ₹50,000 per month."
      ],
      "metadata": {
        "id": "lUCifymhKTE-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.  What are the different types of data in statistics\n",
        "Ans - In statistics, data can be classified into different types based on nature, measurement level, and structure.\n",
        "1. Based on Nature of Data\n",
        "A. Qualitative Data (Categorical Data)\n",
        "Describes attributes or categories; not numerical.\n",
        "Nominal Data: Categories with no inherent order.\n",
        "Examples: Gender (Male/Female), Blood type (A, B, AB, O)\n",
        "Ordinal Data: Categories with a logical order, but intervals are not uniform.\n",
        "Examples: Education level (High School, Bachelor’s, Master’s), Customer satisfaction (Low to High)\n",
        "B. Quantitative Data (Numerical Data)\n",
        "Represents numerical values and can be measured or counted.\n",
        "Discrete Data: Countable and finite values.\n",
        "Examples: Number of children, Number of cars\n",
        "Continuous Data: Can take any value within a range and is measurable.\n",
        "Examples: Height, Weight, Temperature\n",
        " 2. Based on Level of Measurement (Measurement Scales)\n",
        "Nominal Scale – Categorizes without order\n",
        "Example: Types of fruits (Apple, Banana, Mango)\n",
        "Ordinal Scale – Categorizes with a meaningful order\n",
        "Example: Movie ratings (Poor, Average, Good, Excellent)\n",
        "Interval Scale – Ordered, with equal intervals, no true zero\n",
        "Example: Temperature in Celsius (20°C, 30°C, etc.)\n",
        "Ratio Scale – Ordered, equal intervals, and has a true zero\n",
        "Example: Height (0 cm means absence of height), Income, Age\n"
      ],
      "metadata": {
        "id": "Bu2kg-K5K40V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.  Explain nominal, ordinal, interval, and ratio levels of measurement\n",
        "Ans - 1. Nominal Level\n",
        "Definition:\n",
        "The nominal level is the lowest level of measurement. It consists of data that are used for labeling or categorizing observations. The categories are distinct but have no specific order or ranking.\n",
        " 2. Ordinal Level\n",
        "Definition:\n",
        "The ordinal level of measurement involves data that can be ordered or ranked. However, the interval between ranks is not necessarily equal.\n",
        " 3. Interval Level\n",
        "Definition:\n",
        "The interval level of measurement involves data with ordered categories, and the differences between values are meaningful. However, the interval scale has no true zero point—a value of zero does not mean the absence of the property being measured.\n",
        " 4. Ratio Level\n",
        "Definition:\n",
        "The ratio level is the highest level of measurement. It has all the features of the interval level, but with an additional true zero point. The true zero means the absence of the property being measured, and all arithmetic operations (addition, subtraction, multiplication, and division) are valid."
      ],
      "metadata": {
        "id": "iARxP-OULm6R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11.  What is the measure of central tendency\n",
        "Ans - Measure of Central Tendency\n",
        "The measure of central tendency is a statistical concept that represents the center or typical value of a set of data. It provides a single value that summarizes or represents the entire dataset. The three main measures of central tendency are the mean, median, and mode."
      ],
      "metadata": {
        "id": "vo8srvNZMKde"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12.  Define mean, median, and mode\n",
        "Ans - 1. Mean (Arithmetic Average)\n",
        "Definition:\n",
        "The mean is the sum of all the values in a dataset divided by the number of values. It provides a central value that represents the average of the data.\n",
        " 2. Median\n",
        "Definition:\n",
        "The median is the middle value of a dataset when the data points are ordered from smallest to largest. If there is an even number of data points, the median is the average of the two middle values.\n",
        " 3. Mode\n",
        "Definition:\n",
        "The mode is the value that occurs most frequently in a dataset."
      ],
      "metadata": {
        "id": "BB_z5n6UMTz0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What is the significance of the measure of central tendency\n",
        "Ans - Significance of the Measure of Central Tendency\n",
        "The measure of central tendency is significant in statistics because it provides a single value that represents the center or typical value of a dataset. This is crucial for understanding the overall characteristics of a dataset and is widely used in data analysis, decision-making, and comparing different datasets."
      ],
      "metadata": {
        "id": "RP3AxqD0Mui_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14.  What is variance, and how is it calculated\n",
        "Ans - Variance: Definition and Calculation\n",
        "Variance is a statistical measure that describes the spread or dispersion of a set of data points around the mean. It measures how far each data point in the dataset is from the mean and, therefore, gives an idea of the variability within the dataset. A higher variance indicates that the data points are more spread out, while a lower variance means they are closer to the mean.\n"
      ],
      "metadata": {
        "id": "2VbBUqDYNAKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15.  What is standard deviation, and why is it important\n",
        "Ans - Standard deviation is a statistical measure that quantifies the amount of variation or dispersion of a dataset. It indicates how much individual data points deviate from the mean (average) of the dataset. A low standard deviation means that the data points are close to the mean, while a high standard deviation indicates that the data points are spread out over a wide range of values.\n"
      ],
      "metadata": {
        "id": "qS7pMZccNllL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16.   Define and explain the term range in statistics\n",
        "Ans - Range is a simple measure of spread or dispersion in a dataset. It is calculated by finding the difference between the highest and lowest values in the dataset. The range gives a basic idea of how wide or narrow the data is, showing the extent of variability in the values.\n",
        "\n"
      ],
      "metadata": {
        "id": "fxTmtAzDOOlG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17.  What is the difference between variance and standard deviation\n",
        "Ans - Both variance and standard deviation are measures of dispersion in a dataset, meaning they tell us how much the data points deviate from the mean. Although they are related, there are important differences in their interpretation and application.\n",
        " * Variance:\n",
        "Variance measures the average squared deviation of each data point from the mean. In other words, it calculates how far each data point is from the mean, squares the differences, and then averages them. The result is in squared units of the original data\n",
        "* Standard Deviation:\n",
        "Standard deviation is the square root of the variance. It provides a measure of how spread out the data is, but unlike variance, it is in the same units as the original data. This makes standard deviation more interpretable in the context of the data since it is expressed in the same scale as the dataset."
      ],
      "metadata": {
        "id": "mUNexQBUOY3t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What is skewness in a dataset\n",
        "Ans - Skewness is a measure of the asymmetry or lack of symmetry in a dataset’s distribution. It indicates whether the data points are more concentrated on one side of the mean or whether the distribution is \"skewed\" toward either the left or the right."
      ],
      "metadata": {
        "id": "LjaOCN6oOuY1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19.  What does it mean if a dataset is positively or negatively skewed\n",
        "Ans - When a dataset is described as positively skewed or negatively skewed, it refers to the direction of the asymmetry or skewness in the data distribution.\n",
        " 1. Positively Skewed (Right Skewed)\n",
        "A positively skewed (or right-skewed) dataset has a longer tail on the right side. This means that while most of the data points are clustered around the lower values (left side), there are a few extremely high values that stretch the distribution toward the right.\n",
        " 2. Negatively Skewed (Left Skewed)\n",
        "A negatively skewed (or left-skewed) dataset has a longer tail on the left side. This means that while most of the data points are clustered around the higher values (right side), there are a few extremely low values that stretch the distribution toward the left.\n"
      ],
      "metadata": {
        "id": "zNBDNKciO2vW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20.  Define and explain kurtosis\n",
        "Ans - Kurtosis is a statistical measure that describes the shape of a dataset's distribution, specifically focusing on the tailedness (how heavy or light the tails are) and the peakedness (how sharp or flat the peak is) relative to a normal distribution. In simpler terms, kurtosis indicates how much of the data is concentrated in the tails and the center compared to a standard normal distribution."
      ],
      "metadata": {
        "id": "3BF1uz_APXz8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21.  What is the purpose of covariance\n",
        "Ans- Covariance is a statistical measure that indicates the relationship between the variability of two random variables. Specifically, it tells us whether two variables increase or decrease together or whether they move in opposite directions.\n"
      ],
      "metadata": {
        "id": "38qXEe_1Pgng"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. What does correlation measure in statistics\n",
        "Ans - Correlation in statistics is a measure of the strength and direction of the relationship between two variables. It indicates how one variable changes in relation to another, whether they tend to move together (positively or negatively) or if there is no predictable relationship between them."
      ],
      "metadata": {
        "id": "EAk-xNIaPsvA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. What is the difference between covariance and correlation\n",
        "Ans  - Both covariance and correlation measure the relationship between two variables, but they differ in the way they quantify the relationship and the information they provide.\n",
        "* Covariance measures the degree to which two variables change together. It shows whether an increase in one variable leads to an increase (or decrease) in the other variable.\n",
        "It provides a directional relationship (positive or negative), but does not provide a clear sense of the strength of the relationship.\n",
        " * Correlation is a standardized measure of the strength and direction of the relationship between two variables. It quantifies how strongly two variables are related, ranging from −1 (perfect negative correlation) to +1 (perfect positive correlation).\n",
        "Correlation is the normalized version of covariance, meaning it has no units and is easier to interpret across different datasets."
      ],
      "metadata": {
        "id": "mYL-t4hfP0EU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "24.  What are some real-world applications of statistics?\n",
        "Ans - Statistics is a versatile field with a wide range of real-world applications across various industries and disciplines.\n",
        "* 1. Business and Marketing\n",
        "Market Research:\n",
        "Statistics is used to gather and analyze consumer behavior, preferences, and buying patterns. Surveys, focus groups, and data analysis help companies develop targeted marketing strategies.\n",
        "* 2. Healthcare and Medicine\n",
        "Clinical Trials:\n",
        "Statistics plays a crucial role in the design, analysis, and interpretation of clinical trials for new drugs or medical treatments. This helps determine the efficacy and safety of drugs.\n",
        "* 3. Education\n",
        "Standardized Testing:\n",
        "Statistics is widely used in the design and analysis of standardized tests (e.g., SAT, GRE, etc.) to assess academic performance, reliability, and validity of the test scores."
      ],
      "metadata": {
        "id": "tseD0LogQUGe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "                    **Practical**\n",
        "                  \n"
      ],
      "metadata": {
        "id": "vTSNYp8AQxDa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.  How do you calculate the mean, median, and mode of a dataset\n"
      ],
      "metadata": {
        "id": "FbEWhhELQ4vO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "data = [1, 2, 2, 3, 4, 5, 5, 5, 6, 7]\n",
        "\n",
        "mean = np.mean(data)\n",
        "print(\"Mean:\", mean)\n",
        "\n",
        "median = np.median(data)\n",
        "print(\"Median:\", median)\n",
        "\n",
        "mode = stats.mode(data)\n",
        "print(\"Mode:\", mode.mode[0])\n",
        "\n"
      ],
      "metadata": {
        "id": "fJoEtdiPQ8ON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.  Write a Python program to compute the variance and standard deviation of a dataset\n"
      ],
      "metadata": {
        "id": "fJsknXaBRRr5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "data = [1, 2, 2, 3, 4, 5, 5, 5, 6, 7]\n",
        "\n",
        "variance = np.var(data)\n",
        "print(\"Variance:\", variance)\n",
        "\n",
        "std_deviation = np.std(data)\n",
        "print(\"Standard Deviation:\", std_deviation)\n"
      ],
      "metadata": {
        "id": "5V2PLHW_RVDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.  Create a dataset and classify it into nominal, ordinal, interval, and ratio types\n"
      ],
      "metadata": {
        "id": "ULlZPCiYRiQX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = {\n",
        "    'Student ID': [1, 2, 3, 4, 5],\n",
        "    'Name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Edward'],\n",
        "    'Grade': ['A', 'B', 'A', 'C', 'B'],\n",
        "    'Age': [21, 22, 23, 21, 22],\n",
        "    'Height (cm)': [160, 175, 170, 160, 180],\n",
        "    'Income (USD)': [35000, 42000, 50000, 37000, 45000]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "print(\"Dataset:\\n\", df)\n",
        "\n",
        "nominal_columns = ['Name']\n",
        "ordinal_columns = ['Grade']\n",
        "interval_columns = ['Age']\n",
        "ratio_columns = ['Height (cm)', 'Income (USD)']\n",
        "\n",
        "print(\"\\nNominal Data (Categories without order):\", nominal_columns)\n",
        "print(\"Ordinal Data (Ordered categories, but not equal intervals):\", ordinal_columns)\n",
        "print(\"Interval Data (Ordered, equal intervals, but no true zero):\", interval_columns)\n",
        "print(\"Ratio Data (Ordered, equal intervals, with a true zero):\", ratio_columns)\n"
      ],
      "metadata": {
        "id": "ona2rnmyRnjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.  Implement sampling techniques like random sampling and stratified sampling\n"
      ],
      "metadata": {
        "id": "a3oBxSmeR_wV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data = {\n",
        "    'Student ID': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
        "    'Name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Edward', 'Frank', 'Grace', 'Hannah', 'Ivy', 'Jack'],\n",
        "    'Age': [21, 22, 23, 21, 22, 23, 21, 22, 23, 21],\n",
        "    'Grade': ['A', 'B', 'A', 'C', 'B', 'A', 'C', 'B', 'A', 'C']\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "strata = df.groupby('Grade')\n",
        "\n",
        "stratified_sample = strata.apply(lambda x: x.sample(n=1, random_state=1)).reset_index(drop=True)\n",
        "\n",
        "print(\"Stratified Sample:\\n\", stratified_sample)\n"
      ],
      "metadata": {
        "id": "Hv7fw9CaSCvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.  Write a Python function to calculate the range of a dataset\n"
      ],
      "metadata": {
        "id": "toMpurNtSQUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_range(data):\n",
        "    return max(data) - min(data)\n",
        "\n",
        "data = [1, 2, 2, 3, 4, 5, 5, 5, 6, 7]\n",
        "\n",
        "range_value = calculate_range(data)\n",
        "print(\"Range of the dataset:\", range_value)\n"
      ],
      "metadata": {
        "id": "R9P56vo_ST4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Create a dataset and plot its histogram to visualize skewness\n"
      ],
      "metadata": {
        "id": "7OOvAN_3ScTj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "data_positive_skew = np.random.exponential(scale=1, size=1000)\n",
        "\n",
        "data_normal = np.random.normal(loc=0, scale=1, size=1000)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(data_positive_skew, kde=True, color='skyblue', bins=30)\n",
        "plt.title('Positive Skew Data')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.histplot(data_normal, kde=True, color='salmon', bins=30)\n",
        "plt.title('Normal Data (No Skew)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "FHxOC7gCSfkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Calculate skewness and kurtosis of a dataset using Python libraries\n"
      ],
      "metadata": {
        "id": "l6WXfFX_SvPI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import skew, kurtosis\n",
        "\n",
        "data = np.random.exponential(scale=1, size=1000)\n",
        "\n",
        "data_skewness = skew(data)\n",
        "print(\"Skewness of the dataset:\", data_skewness)\n",
        "\n",
        "data_kurtosis = kurtosis(data)\n",
        "print(\"Kurtosis of the dataset:\", data_kurtosis)\n"
      ],
      "metadata": {
        "id": "ixkMoZGfSyPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Generate a dataset and demonstrate positive and negative skewness\n"
      ],
      "metadata": {
        "id": "KgX8i4tYS6Fq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import skew\n",
        "\n",
        "positive_skew_data = np.random.exponential(scale=1, size=1000)\n",
        "\n",
        "negative_skew_data = np.random.lognormal(mean=0, sigma=1, size=1000)\n",
        "\n",
        "positive_skew_value = skew(positive_skew_data)\n",
        "negative_skew_value = skew(negative_skew_data)\n",
        "\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(positive_skew_data, kde=True, color='skyblue', bins=30)\n",
        "plt.title(f\"Positive Skew - Skewness: {positive_skew_value:.2f}\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.histplot(negative_skew_data, kde=True, color='salmon', bins=30)\n",
        "plt.title(f\"Negative Skew - Skewness: {negative_skew_value:.2f}\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "pt6j8DFLS9DM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Write a Python script to calculate covariance between two datasets\n"
      ],
      "metadata": {
        "id": "hlyIM-v0TMPw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "data1 = np.array([1, 2, 3, 4, 5])\n",
        "data2 = np.array([5, 4, 3, 2, 1])\n",
        "\n",
        "cov_matrix = np.cov(data1, data2)\n",
        "\n",
        "cov_value = cov_matrix[0, 1]\n",
        "\n",
        "print(\"Covariance between data1 and data2:\", cov_value)\n"
      ],
      "metadata": {
        "id": "SU_CaQm-TRn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Write a Python script to calculate the correlation coefficient between two datasets\n"
      ],
      "metadata": {
        "id": "BuJ-7LblTaRr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "data1 = np.array([1, 2, 3, 4, 5])\n",
        "data2 = np.array([5, 4, 3, 2, 1])\n",
        "\n",
        "corr_matrix = np.corrcoef(data1, data2)\n",
        "\n",
        "corr_value = corr_matrix[0, 1]\n",
        "\n",
        "print(\"Correlation coefficient between data1 and data2:\", corr_value)\n"
      ],
      "metadata": {
        "id": "SQA5MOj5Td96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Create a scatter plot to visualize the relationship between two variables\n"
      ],
      "metadata": {
        "id": "DGxNtWUGTmOY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "data1 = np.array([1, 2, 3, 4, 5])\n",
        "data2 = np.array([5, 4, 3, 2, 1])\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(data1, data2, color='blue', label='Data Points')\n",
        "plt.title(\"Scatter Plot of Data1 vs Data2\")\n",
        "plt.xlabel(\"Data1\")\n",
        "plt.ylabel(\"Data2\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "sns.lmplot(x=\"data1\", y=\"data2\", data={'data1': data1, 'data2': data2}, scatter_kws={'color': 'blue'})\n",
        "plt.title(\"Scatter Plot with Regression Line (Seaborn)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mrx_IEzCTr02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. Implement and compare simple random sampling and systematic sampling\n"
      ],
      "metadata": {
        "id": "5V0NYPSVT4X0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "population_size = 1000\n",
        "population = np.random.randint(1, 101, population_size)\n",
        "\n",
        "\n",
        "sample_size_srs = 100\n",
        "srs_sample = np.random.choice(population, size=sample_size_srs, replace=False)\n",
        "\n",
        "start = np.random.randint(0, population_size // sample_size_srs)\n",
        "step = population_size // sample_size_srs\n",
        "systematic_sample = population[start::step]\n",
        "\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    'Population': population,\n",
        "    'SRS Sample': np.isin(population, srs_sample),\n",
        "    'Systematic Sample': np.isin(population, systematic_sample)\n",
        "})\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(range(population_size), df['SRS Sample'], color='blue', label='SRS Sample', alpha=0.5)\n",
        "plt.title(\"Simple Random Sampling\")\n",
        "plt.xlabel(\"Index\")\n",
        "plt.ylabel(\"Sampled\")\n",
        "plt.yticks([0, 1], ['Not Sampled', 'Sampled'])\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(range(population_size), df['Systematic Sample'], color='red', label='Systematic Sample', alpha=0.5)\n",
        "plt.title(\"Systematic Sampling\")\n",
        "plt.xlabel(\"Index\")\n",
        "plt.ylabel(\"Sampled\")\n",
        "plt.yticks([0, 1], ['Not Sampled', 'Sampled'])\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"SRS Sample (first 10 elements): {srs_sample[:10]}\")\n",
        "print(f\"Systematic Sample (first 10 elements): {systematic_sample[:10]}\")\n"
      ],
      "metadata": {
        "id": "h5F3oHQ9T8KK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. Calculate the mean, median, and mode of grouped data\n"
      ],
      "metadata": {
        "id": "u6IQ27_AUZZi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "classes = [(0, 10), (10, 20), (20, 30), (30, 40), (40, 50)]\n",
        "frequencies = [5, 15, 25, 30, 25]\n",
        "\n",
        "midpoints = [(low + high) / 2 for low, high in classes]\n",
        "\n",
        "total_frequency = sum(frequencies)\n",
        "mean = sum(f * m for f, m in zip(frequencies, midpoints)) / total_frequency\n",
        "print(f\"Mean: {mean:.2f}\")\n",
        "\n",
        "cumulative_frequencies = np.cumsum(frequencies)\n",
        "\n",
        "median_class_index = np.where(cumulative_frequencies >= total_frequency / 2)[0][0]\n",
        "\n",
        "L = classes[median_class_index][0]\n",
        "CF = cumulative_frequencies[median_class_index - 1] if median_class_index > 0 else 0\n",
        "f = frequencies[median_class_index]\n",
        "h = classes[median_class_index][1] - classes[median_class_index][0]\n",
        "\n",
        "median = L + ((total_frequency / 2 - CF) / f) * h\n",
        "print(f\"Median: {median:.2f}\")\n",
        "\n",
        "mode_class_index = np.argmax(frequencies)\n",
        "\n",
        "f1 = frequencies[mode_class_index]\n"
      ],
      "metadata": {
        "id": "HzhkpuTrUcoz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. Simulate data using Python and calculate its central tendency and dispersion.\n"
      ],
      "metadata": {
        "id": "S-mxjH71UxxR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "\n",
        "np.random.seed(42)\n",
        "data = np.random.normal(loc=50, scale=10, size=1000)\n",
        "\n",
        "data_series = pd.Series(data)\n",
        "\n",
        "mean = data_series.mean()\n",
        "median = data_series.median()\n",
        "mode = data_series.mode()[0]\n",
        "\n",
        "data_range = data_series.max() - data_series.min()\n",
        "variance = data_series.var()\n",
        "std_dev = data_series.std()\n",
        "\n",
        "print(\"Central Tendency:\")\n",
        "print(f\"Mean: {mean:.2f}\")\n",
        "print(f\"Median: {median:.2f}\")\n",
        "print(f\"Mode: {mode:.2f}\")\n",
        "\n",
        "print(\"\\nDispersion:\")\n",
        "print(f\"Range: {data_range:.2f}\")\n",
        "print(f\"Variance: {variance:.2f}\")\n",
        "print(f\"Standard Deviation: {std_dev:.2f}\")\n"
      ],
      "metadata": {
        "id": "i6wRcOrQU4BW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "15.Use NumPy or pandas to summarize a dataset’s descriptive statistics\n"
      ],
      "metadata": {
        "id": "sv5cwYA0VHD2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "np.random.seed(0)\n",
        "data = np.random.normal(loc=60, scale=15, size=1000)\n",
        "\n",
        "data_series = pd.Series(data)\n",
        "\n",
        "summary = data_series.describe()\n",
        "\n",
        "mode = data_series.mode()[0]\n",
        "variance = data_series.var()\n",
        "data_range = data_series.max() - data_series.min()\n",
        "\n",
        "print(\"Descriptive Statistics:\")\n",
        "print(summary)\n",
        "print(f\"Mode: {mode:.2f}\")\n",
        "print(f\"Variance: {variance:.2f}\")\n",
        "print(f\"Range: {data_range:.2f}\")\n"
      ],
      "metadata": {
        "id": "1-FVGwulVJy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. Plot a boxplot to understand the spread and identify outliers\n"
      ],
      "metadata": {
        "id": "hLbv4baPVb2Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "np.random.seed(10)\n",
        "data = np.random.normal(loc=50, scale=12, size=500)\n",
        "\n",
        "df = pd.DataFrame({'Values': data})\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "sns.boxplot(x='Values', data=df, color='skyblue')\n",
        "plt.title('Boxplot to Visualize Spread and Outliers')\n",
        "plt.xlabel('Values')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "9GKeNhjCVecM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. Calculate the interquartile range (IQR) of a dataset\n"
      ],
      "metadata": {
        "id": "Gjq3G963Vlaa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "np.random.seed(1)\n",
        "data = np.random.normal(loc=100, scale=15, size=1000)\n",
        "\n",
        "Q1 = np.percentile(data, 25)\n",
        "Q3 = np.percentile(data, 75)\n",
        "\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "print(f\"Q1 (25th percentile): {Q1:.2f}\")\n",
        "print(f\"Q3 (75th percentile): {Q3:.2f}\")\n",
        "print(f\"IQR (Q3 - Q1): {IQR:.2f}\")\n"
      ],
      "metadata": {
        "id": "BWhtTQQKVp7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. Implement Z-score normalization and explain its significance\n"
      ],
      "metadata": {
        "id": "Y4CkPkiMV0Q6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "np.random.seed(42)\n",
        "data = np.random.normal(loc=70, scale=15, size=100)\n",
        "\n",
        "data_series = pd.Series(data)\n",
        "\n",
        "mean = data_series.mean()\n",
        "std_dev = data_series.std()\n",
        "\n",
        "z_scores = (data_series - mean) / std_dev\n",
        "\n",
        "print(\"First 5 Z-scores:\")\n",
        "print(z_scores.head())\n"
      ],
      "metadata": {
        "id": "4F9U56LEV3Yg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. Compare two datasets using their standard deviations\n"
      ],
      "metadata": {
        "id": "znS7FuYfWBk7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "data_a = np.random.normal(loc=50, scale=5, size=1000)\n",
        "\n",
        "data_b = np.random.normal(loc=50, scale=20, size=1000)\n",
        "\n",
        "std_a = np.std(data_a)\n",
        "std_b = np.std(data_b)\n",
        "\n",
        "print(f\"Standard Deviation of Dataset A: {std_a:.2f}\")\n",
        "print(f\"Standard Deviation of Dataset B: {std_b:.2f}\")\n",
        "\n",
        "if std_a < std_b:\n",
        "    print(\"➡ Dataset A is more consistent (less spread).\")\n",
        "else:\n",
        "    print(\"➡ Dataset B is more consistent (less spread).\")\n"
      ],
      "metadata": {
        "id": "Xviff5ghWEbY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. Write a Python program to visualize covariance using a heatmap\n"
      ],
      "metadata": {
        "id": "DUAXA_28WPzY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(0)\n",
        "x = np.random.normal(50, 10, 100)\n",
        "y = x + np.random.normal(0, 5, 100)\n",
        "z = np.random.normal(30, 15, 100)\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    'X': x,\n",
        "    'Y': y,\n",
        "    'Z': z\n",
        "})\n",
        "\n",
        "cov_matrix = df.cov()\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cov_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", square=True)\n",
        "plt.title(\"Covariance Heatmap\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "evy4zEnoWSMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. Use seaborn to create a correlation matrix for a dataset\n"
      ],
      "metadata": {
        "id": "tmNUNDHRWfhS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(1)\n",
        "data = {\n",
        "    'A': np.random.normal(50, 10, 100),\n",
        "    'B': np.random.normal(30, 5, 100),\n",
        "    'C': np.random.normal(100, 20, 100),\n",
        "    'D': np.random.normal(70, 15, 100)\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "corr_matrix = df.corr()\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5, square=True)\n",
        "plt.title(\"Correlation Matrix Heatmap\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "PVcdmJylWiXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. Generate a dataset and implement both variance and standard deviation computations\n"
      ],
      "metadata": {
        "id": "sX5MEnRTWqb-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "np.random.seed(42)\n",
        "data = np.random.normal(loc=100, scale=15, size=100)\n",
        "\n",
        "data_series = pd.Series(data)\n",
        "\n",
        "np_variance = np.var(data, ddof=1)\n",
        "np_std_dev = np.std(data, ddof=1)\n",
        "\n",
        "pd_variance = data_series.var()\n",
        "pd_std_dev = data_series.std()\n",
        "\n",
        "print(\"Using NumPy:\")\n",
        "print(f\"Variance: {np_variance:.2f}\")\n",
        "print(f\"Standard Deviation: {np_std_dev:.2f}\\n\")\n",
        "\n",
        "print(\"Using pandas:\")\n",
        "print(f\"Variance: {pd_variance:.2f}\")\n",
        "print(f\"Standard Deviation: {pd_std_dev:.2f}\")\n"
      ],
      "metadata": {
        "id": "sM8lmHAmWtR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. Visualize skewness and kurtosis using Python libraries like matplotlib or seaborn\n"
      ],
      "metadata": {
        "id": "V5GCh18FW7ut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import skew, kurtosis\n",
        "\n",
        "np.random.seed(0)\n",
        "data = np.random.exponential(scale=2, size=1000)\n",
        "\n",
        "data_series = pd.Series(data)\n",
        "\n",
        "data_skew = skew(data)\n",
        "data_kurt = kurtosis(data)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(data_series, kde=True, color='skyblue', bins=30)\n",
        "plt.title(\"Histogram with KDE\\nSkewness and Kurtosis Visualization\", fontsize=14)\n",
        "plt.xlabel(\"Values\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "\n",
        "plt.text(data_series.mean() + 1, 150,\n",
        "         f\"Skewness: {data_skew:.2f}\\nKurtosis: {data_kurt:.2f}\",\n",
        "         fontsize=12, bbox=dict(facecolor='white', edgecolor='black'))\n",
        "\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "iK2MWK_tW_BM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. Implement the Pearson and Spearman correlation coefficients for a dataset.\n"
      ],
      "metadata": {
        "id": "v6s96xDDXMcX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import pearsonr, spearmanr\n",
        "\n",
        "np.random.seed(0)\n",
        "x = np.random.normal(50, 10, 100)\n",
        "y = 2 * x + np.random.normal(0, 5, 100)\n",
        "\n",
        "df = pd.DataFrame({'X': x, 'Y': y})\n",
        "\n",
        "pearson_corr, pearson_p_value = pearsonr(df['X'], df['Y'])\n",
        "\n",
        "spearman_corr, spearman_p_value = spearmanr(df['X'], df['Y'])\n",
        "\n",
        "print(f\"Pearson Correlation: {pearson_corr:.2f} (p-value: {pearson_p_value:.3f})\")\n",
        "print(f\"Spearman Correlation: {spearman_corr:.2f} (p-value: {spearman_p_value:.3f})\")\n"
      ],
      "metadata": {
        "id": "mFquFcgkXPt9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}